{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global_Wheat_Detection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height   source      x      y      w      h\n",
       "0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n",
       "1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n",
       "2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n",
       "3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n",
       "4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marking = pd.read_csv('global-wheat-detection_data/train.csv')\n",
    "\n",
    "bboxs = np.stack(marking['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, column in enumerate(['x', 'y', 'w', 'h']):\n",
    "    marking[column] = bboxs[:,i]\n",
    "marking.drop(columns=['bbox'], inplace=True)\n",
    "\n",
    "marking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "marking['source'].hist(bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuyihuo/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "df_folds = marking[['image_id']].copy()\n",
    "df_folds.loc[:, 'bbox_count'] = 1\n",
    "df_folds = df_folds.groupby('image_id').count()\n",
    "df_folds.loc[:, 'source'] = marking[['image_id', 'source']].groupby('image_id').min()['source']\n",
    "df_folds.loc[:, 'stratify_group'] = np.char.add(\n",
    "    df_folds['source'].values.astype(str),\n",
    "    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n",
    ")\n",
    "df_folds.loc[:, 'fold'] = 0\n",
    "\n",
    "for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n",
    "    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbox_count</th>\n",
       "      <th>source</th>\n",
       "      <th>stratify_group</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00333207f</th>\n",
       "      <td>55</td>\n",
       "      <td>arvalis_1</td>\n",
       "      <td>arvalis_1_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005b0d8bb</th>\n",
       "      <td>20</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>usask_1_1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>006a994f7</th>\n",
       "      <td>25</td>\n",
       "      <td>inrae_1</td>\n",
       "      <td>inrae_1_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00764ad5d</th>\n",
       "      <td>41</td>\n",
       "      <td>inrae_1</td>\n",
       "      <td>inrae_1_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00b5fefed</th>\n",
       "      <td>25</td>\n",
       "      <td>arvalis_3</td>\n",
       "      <td>arvalis_3_1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bbox_count     source stratify_group  fold\n",
       "image_id                                             \n",
       "00333207f          55  arvalis_1    arvalis_1_3     1\n",
       "005b0d8bb          20    usask_1      usask_1_1     3\n",
       "006a994f7          25    inrae_1      inrae_1_1     1\n",
       "00764ad5d          41    inrae_1      inrae_1_2     0\n",
       "00b5fefed          25  arvalis_3    arvalis_3_1     3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = 'global-wheat-detection_data/test'\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_ids, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "        return image, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global-wheat-detection_data/train/75d62c43e.jpg'\n",
      " 'global-wheat-detection_data/train/a7436f5c7.jpg'\n",
      " 'global-wheat-detection_data/train/3049b3745.jpg' ...\n",
      " 'global-wheat-detection_data/train/3b4f7ba0a.jpg'\n",
      " 'global-wheat-detection_data/train/be11c4e40.jpg'\n",
      " 'global-wheat-detection_data/train/457752437.jpg']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob \n",
    "# load filenames \n",
    "train = np.array(glob(\"global-wheat-detection_data/train/*\"))\n",
    "test = np.array(glob(\"global-wheat-detection_data/test/*\"))\n",
    "print(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = DatasetRetriever(\n",
    "    image_ids=np.array([path.split('/')[-1][:-4] for path in test]),\n",
    "    transforms=get_valid_transforms()\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "\n",
    "def load_net(checkpoint_path):\n",
    "    net = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "    num_classes = 2  # 1 class (wheat) + background\n",
    "    # get number of input features for the classifier\n",
    "    in_features = net.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    net.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    net.load_state_dict(checkpoint)\n",
    "    \n",
    "    net = net\n",
    "    net.eval()\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    load_net('pretrained_model/fold0-best1.bin'),\n",
    "    load_net('pretrained_model/fold1-best1.bin'),\n",
    "    load_net('pretrained_model/fold2-best1.bin'),\n",
    "    load_net('pretrained_model/fold3-best1.bin'),\n",
    "    load_net('pretrained_model/fold4-best1.bin'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_boxes import *\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "def make_ensemble_predictions(images):\n",
    "    images = list(image for image in images)    \n",
    "    result = []\n",
    "    for net in models:\n",
    "        outputs = net(images)\n",
    "        result.append(outputs)\n",
    "    return result\n",
    "\n",
    "def run_wbf(predictions, image_index, image_size=512, iou_thr=0.55, skip_box_thr=0.7, weights=None):\n",
    "    boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]) for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-147ee6865102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_wbf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m511\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for j, (images, image_ids) in enumerate(data_loader):\n",
    "    if j > 0:\n",
    "        break\n",
    "predictions = make_ensemble_predictions(images)\n",
    "\n",
    "i = 3\n",
    "sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "boxes = boxes.astype(np.int32).clip(min=0, max=511)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 2)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuyihuo/opt/anaconda3/lib/python3.7/site-packages/ensemble_boxes/ensemble_boxes_wbf.py:83: UserWarning: Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.\n",
      "  warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "\n",
    "for images, image_ids in data_loader:\n",
    "    predictions = make_ensemble_predictions(images)\n",
    "    for i, image in enumerate(images):\n",
    "        boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "        boxes = (boxes*2).astype(np.int32).clip(min=0, max=1023)\n",
    "        image_id = image_ids[i]\n",
    "\n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>0.9991 460 497 78 135 0.9984 106 584 138 85 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53f253011</td>\n",
       "      <td>0.9983 931 202 91 133 0.9973 233 840 114 93 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5a1f0358</td>\n",
       "      <td>0.9972 886 645 86 141 0.9964 544 272 107 113 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cc3532ff6</td>\n",
       "      <td>0.9986 771 827 165 158 0.9986 376 0 82 98 0.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>0.9986 836 450 185 145 0.9982 234 644 93 155 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                   PredictionString\n",
       "0  2fd875eaa  0.9991 460 497 78 135 0.9984 106 584 138 85 0....\n",
       "1  53f253011  0.9983 931 202 91 133 0.9973 233 840 114 93 0....\n",
       "2  f5a1f0358  0.9972 886 645 86 141 0.9964 544 272 107 113 0...\n",
       "3  cc3532ff6  0.9986 771 827 165 158 0.9986 376 0 82 98 0.99...\n",
       "4  51b3e36ab  0.9986 836 450 185 145 0.9982 234 644 93 155 0..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "test_df.to_csv('submission.csv', index=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
